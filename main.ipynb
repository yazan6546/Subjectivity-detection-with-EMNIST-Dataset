{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6omna0i_tZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PKvGc21t5mG",
        "outputId": "fbf5e4f4-de5e-4c11-e6b9-a9456db96d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7gC9RmRdO7eO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiRsRt0IPG-c",
        "outputId": "ec64f45a-03ff-4df4-d9a5-7b1d72f645ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byRRFO16kvRF"
      },
      "outputs": [],
      "source": [
        "# Load the mapping from mapping.txt\n",
        "def load_mapping(mapping_file):\n",
        "    mapping = {}\n",
        "    with open(mapping_file, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split()\n",
        "            mapping[int(key)] = chr(int(value))\n",
        "    return mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPWYS748oK8S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "kSYZtm6rj1Vx",
        "outputId": "5896d782-b3bf-4d6c-a56a-2a835d43268c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Display the first few images along with their ASCII labels\n",
        "def show_images(dataset, mapping, num_images=5):\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        image, label = dataset[i]\n",
        "        image = image.numpy().squeeze()\n",
        "        ascii_label = mapping[label]\n",
        "        axes[i].imshow(image, cmap='gray')\n",
        "        axes[i].set_title(f'Label: {ascii_label}')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_images(dataset, mapping=mapping, num_images=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7qo0opU1uMp"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efXx47iDy2PH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j8cuJB-2n59"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.labels = self.data.iloc[:, 0].values\n",
        "        self.images = self.data.iloc[:, 1:].values.reshape(-1, 28, 28).astype('float32')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Wk9IcGW38VLu",
        "outputId": "d03632e4-f93b-4201-b097-e9e1662c6536"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'batch_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c4c2d792be20>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create DataLoader for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "PATH='drive/MyDrive/Subjectivity'\n",
        "\n",
        "test_csv_file = os.path.join(PATH, 'data', 'characters-test.csv')\n",
        "test_dataset = CharacterDataset(test_csv_file, transform=transform)\n",
        "\n",
        "# Create DataLoader for testing\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9kES4iX8Nt7",
        "outputId": "77bbd806-4b4b-411c-a783-09fdf4cca769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 75.4029469652641%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Evaluate the model on the test dataset\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_dataloader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw3dzbgdxWFI",
        "outputId": "e300581f-34bd-4446-f463-fa4158ad2d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksCCAihC26gy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.labels = self.data.iloc[:, 0].values\n",
        "        self.images = self.data.iloc[:, 1:].values.reshape(-1, 28, 28).astype('float32')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert single-channel to three channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "SGDEBozJHceJ",
        "outputId": "a22d25db-f480-4d89-9297-2fdf0622d260"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'drive/MyDrive/Subjectivity/data/characters.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ee640604d777>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'characters.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-c039cdab01f4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, transform)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCharacterDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Subjectivity/data/characters.csv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PATH='drive/MyDrive/Subjectivity'\n",
        "\n",
        "# Load the dataset\n",
        "csv_file = os.path.join(PATH, 'data', 'characters.csv')\n",
        "train_dataset = CharacterDataset(csv_file, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj8cbat-HXZ1"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_csv_file = os.path.join(PATH, 'data', 'characters-test.csv')\n",
        "\n",
        "test_dataset = CharacterDataset(test_csv_file, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIFQpCgZxTJC"
      },
      "outputs": [],
      "source": [
        "mapping_file = os.path.join(PATH, 'data', 'mapping.txt')\n",
        "mapping = load_mapping(mapping_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VQAGv9xGzRi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to create the model\n",
        "def create_model():\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Move the model to GPU if available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, len(mapping))\n",
        "\n",
        "    # Freeze all layers except the last three\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"layer4\" not in name and \"fc\" not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_dataloader, learning_rate, num_epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(model, learning_rate, batch_size):\n",
        "    model_path = f'resnet18_finetuned_lr{learning_rate}_bs{batch_size}.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'Model saved to {model_path}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy}%')\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ohJHUjNG-6R",
        "outputId": "9be8e242-6fbf-4979-bbad-43c73dcd3382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with learning rate: 0.0001, batch size: 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 150MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different learning rates and batch sizes\n",
        "learning_rates = [0.0001]\n",
        "batch_sizes = [32, 64]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        print(f'Training with learning rate: {lr}, batch size: {bs}')\n",
        "\n",
        "        # Create DataLoader for training\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = create_model()\n",
        "        model = train_model(model, train_dataloader, lr)\n",
        "\n",
        "        # Save the model\n",
        "        save_model(model, lr, bs)\n",
        "\n",
        "        # Evaluate the model\n",
        "        accuracy = evaluate_model(model, test_dataloader)\n",
        "        results.append((lr, bs, accuracy))\n",
        "\n",
        "# Print results\n",
        "for lr, bs, acc in results:\n",
        "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Test Accuracy: {acc}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGRD_XaXPttL"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # Ensure single-channel images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2XgQizIP8Na"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the mapping from mapping.txt\n",
        "def load_mapping(mapping_file):\n",
        "    mapping = {}\n",
        "    with open(mapping_file, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split()\n",
        "            mapping[int(key)] = chr(int(value))\n",
        "    return mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPMyWGzhP1en"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "mapping_file = os.path.join(PATH, 'data', 'mapping.txt')\n",
        "mapping = load_mapping(mapping_file)\n",
        "\n",
        "\n",
        "test_csv_file = os.path.join(PATH, 'data', 'characters-test.csv')\n",
        "\n",
        "test_dataset = CharacterDataset(test_csv_file, transform=transform)\n",
        "\n",
        "PATH='drive/MyDrive/Subjectivity'\n",
        "\n",
        "# Load the dataset\n",
        "csv_file = os.path.join(PATH, 'data', 'characters.csv')\n",
        "train_dataset = CharacterDataset(csv_file, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWBmX75XOxnn",
        "outputId": "cf78e295-ec9d-44c7-d32e-327b869f9856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with learning rate: 0.001, batch size: 64, conv layers: 2, fc layers: 1\n",
            "Epoch [1/10], Loss: 0.7109874288212895\n",
            "Epoch [2/10], Loss: 0.43929021836797827\n",
            "Epoch [3/10], Loss: 0.3753047070750714\n",
            "Epoch [4/10], Loss: 0.32908195918991157\n",
            "Epoch [5/10], Loss: 0.29487128223182124\n",
            "Epoch [6/10], Loss: 0.2628828048984976\n",
            "Epoch [7/10], Loss: 0.23985590739360568\n",
            "Epoch [8/10], Loss: 0.21692587150618967\n",
            "Epoch [9/10], Loss: 0.2002901548199605\n",
            "Epoch [10/10], Loss: 0.18487916001548196\n",
            "Model saved to custom_cnn_lr0.001_bs64_conv2_fc1.pth\n",
            "Test Accuracy: 84.48321719240386%\n",
            "Training with learning rate: 0.001, batch size: 64, conv layers: 2, fc layers: 2\n",
            "Epoch [1/10], Loss: 1.0399270086415573\n",
            "Epoch [2/10], Loss: 0.6561686748884918\n",
            "Epoch [3/10], Loss: 0.580873777547886\n",
            "Epoch [4/10], Loss: 0.5385572908195797\n",
            "Epoch [5/10], Loss: 0.5036691162593651\n",
            "Epoch [6/10], Loss: 0.4781480223887087\n",
            "Epoch [7/10], Loss: 0.4579063706095761\n",
            "Epoch [8/10], Loss: 0.4408851809803897\n",
            "Epoch [9/10], Loss: 0.42345460718555744\n",
            "Epoch [10/10], Loss: 0.4085453230839052\n",
            "Model saved to custom_cnn_lr0.001_bs64_conv2_fc2.pth\n",
            "Test Accuracy: 86.73333687962126%\n",
            "Training with learning rate: 0.001, batch size: 64, conv layers: 3, fc layers: 1\n",
            "Epoch [1/10], Loss: 0.6729717358684784\n",
            "Epoch [2/10], Loss: 0.3871460247381262\n",
            "Epoch [3/10], Loss: 0.33527252651958117\n",
            "Epoch [4/10], Loss: 0.3021107031010256\n",
            "Epoch [5/10], Loss: 0.2734286419147128\n",
            "Epoch [6/10], Loss: 0.24898273409706115\n",
            "Epoch [7/10], Loss: 0.22942847311412212\n",
            "Epoch [8/10], Loss: 0.20745671304894725\n",
            "Epoch [9/10], Loss: 0.19149900855569427\n",
            "Epoch [10/10], Loss: 0.17510826832134652\n",
            "Model saved to custom_cnn_lr0.001_bs64_conv3_fc1.pth\n",
            "Test Accuracy: 86.30246289696261%\n",
            "Training with learning rate: 0.001, batch size: 64, conv layers: 3, fc layers: 2\n",
            "Epoch [1/10], Loss: 1.0658852702098616\n",
            "Epoch [2/10], Loss: 0.58361973937641\n",
            "Epoch [3/10], Loss: 0.5050074685147319\n",
            "Epoch [4/10], Loss: 0.4653498779388564\n",
            "Epoch [5/10], Loss: 0.4332634753891506\n",
            "Epoch [6/10], Loss: 0.4084908420381665\n",
            "Epoch [7/10], Loss: 0.38942824045045094\n",
            "Epoch [8/10], Loss: 0.371563773849473\n",
            "Epoch [9/10], Loss: 0.3577850265549924\n",
            "Epoch [10/10], Loss: 0.3436787023768463\n",
            "Model saved to custom_cnn_lr0.001_bs64_conv3_fc2.pth\n",
            "Test Accuracy: 87.36102984201287%\n",
            "Learning Rate: 0.001, Batch Size: 64, Conv Layers: 2, FC Layers: 1, Test Accuracy: 84.48321719240386%\n",
            "Learning Rate: 0.001, Batch Size: 64, Conv Layers: 2, FC Layers: 2, Test Accuracy: 86.73333687962126%\n",
            "Learning Rate: 0.001, Batch Size: 64, Conv Layers: 3, FC Layers: 1, Test Accuracy: 86.30246289696261%\n",
            "Learning Rate: 0.001, Batch Size: 64, Conv Layers: 3, FC Layers: 2, Test Accuracy: 87.36102984201287%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.labels = self.data.iloc[:, 0].values\n",
        "        self.images = self.data.iloc[:, 1:].values.reshape(-1, 28, 28).astype('float32')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# # Load the training dataset\n",
        "# csv_file = 'characters.csv'\n",
        "# mapping_file = 'mapping.txt'\n",
        "# mapping = load_mapping(mapping_file)\n",
        "# train_dataset = CharacterDataset(csv_file, transform=transform)\n",
        "\n",
        "# # Load the test dataset\n",
        "# test_csv_file = 'characters_test.csv'\n",
        "# test_dataset = CharacterDataset(test_csv_file, transform=transform)\n",
        "\n",
        "# Custom CNN class\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes, num_conv_layers=3, num_fc_layers=2, dropout_rate=0.5):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # Add convolutional layers\n",
        "        in_channels = 1\n",
        "        out_channels = 32\n",
        "        for _ in range(num_conv_layers):\n",
        "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "            self.conv_layers.append(nn.ReLU())\n",
        "            self.conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "            in_channels = out_channels\n",
        "            out_channels *= 2\n",
        "\n",
        "        # Calculate the size of the flattened feature map\n",
        "        self.flattened_size = in_channels * (28 // (2 ** num_conv_layers)) ** 2\n",
        "\n",
        "        # Add fully connected layers\n",
        "        in_features = self.flattened_size\n",
        "        out_features = 256\n",
        "        for _ in range(num_fc_layers - 1):\n",
        "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
        "            self.fc_layers.append(nn.ReLU())\n",
        "            self.fc_layers.append(nn.Dropout(dropout_rate))\n",
        "            in_features = out_features\n",
        "\n",
        "        # Add the final fully connected layer\n",
        "        self.fc_layers.append(nn.Linear(in_features, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x)\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        for layer in self.fc_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_dataloader, learning_rate, num_epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(model, learning_rate, batch_size, num_conv_layers, num_fc_layers):\n",
        "    model_path = f'custom_cnn_lr{learning_rate}_bs{batch_size}_conv{num_conv_layers}_fc{num_fc_layers}.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'Model saved to {model_path}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Experiment with different hyperparameters\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [64]\n",
        "num_conv_layers_list = [2, 3]\n",
        "num_fc_layers_list = [1, 2]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        for num_conv_layers in num_conv_layers_list:\n",
        "            for num_fc_layers in num_fc_layers_list:\n",
        "                print(f'Training with learning rate: {lr}, batch size: {bs}, conv layers: {num_conv_layers}, fc layers: {num_fc_layers}')\n",
        "\n",
        "                # Create DataLoader for training\n",
        "                train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "                test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "                # Create and train the model\n",
        "                model = CustomCNN(num_classes=len(mapping), num_conv_layers=num_conv_layers, num_fc_layers=num_fc_layers)\n",
        "                model = train_model(model, train_dataloader, lr)\n",
        "\n",
        "                # Save the model\n",
        "                save_model(model, lr, bs, num_conv_layers, num_fc_layers)\n",
        "\n",
        "                # Evaluate the model\n",
        "                accuracy = evaluate_model(model, test_dataloader)\n",
        "                results.append((lr, bs, num_conv_layers, num_fc_layers, accuracy))\n",
        "\n",
        "# Print results\n",
        "for lr, bs, num_conv_layers, num_fc_layers, acc in results:\n",
        "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Conv Layers: {num_conv_layers}, FC Layers: {num_fc_layers}, Test Accuracy: {acc}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i5Y5KNDhcs0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bagFGdCUhcs1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uwZF5QMmhcs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the English tokenizer, POS tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Em26dAb1hcs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Custom Dataset class for text data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tsv_file, transform=None):\n",
        "        self.data = pd.read_csv(tsv_file, delimiter='\\t')\n",
        "        self.sentences = self.data['sentence'].values\n",
        "        self.labels = LabelEncoder().fit_transform(self.data['label'].values)\n",
        "        self.transform = transform\n",
        "        self.vocab = self.build_vocab(self.sentences)\n",
        "        self.word2idx = {word: idx for idx, (word, _) in enumerate(self.vocab.items(), start=1)}\n",
        "        self.word2idx['<PAD>'] = 0\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        counter = Counter()\n",
        "        for sentence in sentences:\n",
        "            doc = nlp(sentence)\n",
        "            counter.update([token.text.lower() for token in doc if not token.is_stop])\n",
        "        return counter\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        doc = nlp(sentence)\n",
        "        tokens = [token.text.lower() for token in doc if not token.is_stop]\n",
        "        indices = [self.word2idx[token] for token in tokens]\n",
        "        return torch.tensor(indices), torch.tensor(label)\n",
        "\n",
        "# Function to collate data samples into batches\n",
        "def collate_fn(batch):\n",
        "    sentences, labels = zip(*batch)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return sentences_padded, labels\n",
        "\n",
        "# Custom LSTM class\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_dataloader, learning_rate, num_epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for sentences, labels in train_dataloader:\n",
        "            sentences, labels = sentences.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sentences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(model, learning_rate, batch_size, hidden_size, num_layers):\n",
        "    model_path = f'lstm_model_lr{learning_rate}_bs{batch_size}_hs{hidden_size}_nl{num_layers}.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'Model saved to {model_path}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for sentences, labels in test_dataloader:\n",
        "            sentences, labels = sentences.to(device), labels.to(device)\n",
        "            outputs = model(sentences)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy}%')\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R1kQtJ_Fhcs4"
      },
      "outputs": [],
      "source": [
        "PATH='drive/MyDrive/Subjectivity'\n",
        "\n",
        "train_tsv_file = os.path.join(PATH, 'data', 'train_en.tsv')\n",
        "test_tsv_file = os.path.join(PATH, 'data', 'test_en_gold.tsv')\n",
        "\n",
        "train_dataset = TextDataset(train_tsv_file)\n",
        "test_dataset = TextDataset(test_tsv_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhdBPgnQhcs7",
        "outputId": "91b7d08d-08e3-42ac-8875-bc648a4239b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with learning rate: 0.001, batch size: 32, hidden size: 128, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6800953470743619\n",
            "Epoch [2/10], Loss: 0.6550666162600884\n",
            "Epoch [3/10], Loss: 0.6463322845789102\n",
            "Epoch [4/10], Loss: 0.643368682036033\n",
            "Epoch [5/10], Loss: 0.6354010242682236\n",
            "Epoch [6/10], Loss: 0.6335464280385238\n",
            "Epoch [7/10], Loss: 0.619149930202044\n",
            "Epoch [8/10], Loss: 0.5887337865737768\n",
            "Epoch [9/10], Loss: 0.6263198153330729\n",
            "Epoch [10/10], Loss: 0.5670859607366415\n",
            "Model saved to lstm_model_lr0.001_bs32_hs128_nl1.pth\n",
            "Test Accuracy: 48.97119341563786%\n",
            "Training with learning rate: 0.001, batch size: 32, hidden size: 128, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6610184495265667\n",
            "Epoch [2/10], Loss: 0.6545133705322559\n",
            "Epoch [3/10], Loss: 0.650562754044166\n",
            "Epoch [4/10], Loss: 0.6412010720142951\n",
            "Epoch [5/10], Loss: 0.6400659588667063\n",
            "Epoch [6/10], Loss: 0.6317349362831849\n",
            "Epoch [7/10], Loss: 0.6184805264839759\n",
            "Epoch [8/10], Loss: 0.6120812159318191\n",
            "Epoch [9/10], Loss: 0.5491719498084142\n",
            "Epoch [10/10], Loss: 0.4665390310379175\n",
            "Model saved to lstm_model_lr0.001_bs32_hs128_nl2.pth\n",
            "Test Accuracy: 51.851851851851855%\n",
            "Training with learning rate: 0.001, batch size: 32, hidden size: 256, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6614228968436902\n",
            "Epoch [2/10], Loss: 0.6574671039214501\n",
            "Epoch [3/10], Loss: 0.6440329322448144\n",
            "Epoch [4/10], Loss: 0.6408654038722699\n",
            "Epoch [5/10], Loss: 0.63946533203125\n",
            "Epoch [6/10], Loss: 0.637402009505492\n",
            "Epoch [7/10], Loss: 0.6149062319443777\n",
            "Epoch [8/10], Loss: 0.5951965989974829\n",
            "Epoch [9/10], Loss: 0.6150170266628265\n",
            "Epoch [10/10], Loss: 0.6318906453939584\n",
            "Model saved to lstm_model_lr0.001_bs32_hs256_nl1.pth\n",
            "Test Accuracy: 46.91358024691358%\n",
            "Training with learning rate: 0.001, batch size: 32, hidden size: 256, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6573676879589374\n",
            "Epoch [2/10], Loss: 0.6534350835360013\n",
            "Epoch [3/10], Loss: 0.6496704106147473\n",
            "Epoch [4/10], Loss: 0.6343010320113256\n",
            "Epoch [5/10], Loss: 0.648012651846959\n",
            "Epoch [6/10], Loss: 0.6083265290810511\n",
            "Epoch [7/10], Loss: 0.6001945115052737\n",
            "Epoch [8/10], Loss: 0.5288963902455109\n",
            "Epoch [9/10], Loss: 0.46450619055674625\n",
            "Epoch [10/10], Loss: 0.43683855636761737\n",
            "Model saved to lstm_model_lr0.001_bs32_hs256_nl2.pth\n",
            "Test Accuracy: 44.8559670781893%\n",
            "Training with learning rate: 0.001, batch size: 64, hidden size: 128, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6687105022943937\n",
            "Epoch [2/10], Loss: 0.6532975664505591\n",
            "Epoch [3/10], Loss: 0.6496292627774752\n",
            "Epoch [4/10], Loss: 0.6509555257283725\n",
            "Epoch [5/10], Loss: 0.6460669865975013\n",
            "Epoch [6/10], Loss: 0.6445033458562998\n",
            "Epoch [7/10], Loss: 0.6421458812860342\n",
            "Epoch [8/10], Loss: 0.6436641032879169\n",
            "Epoch [9/10], Loss: 0.6387266104037945\n",
            "Epoch [10/10], Loss: 0.6355806497427133\n",
            "Model saved to lstm_model_lr0.001_bs64_hs128_nl1.pth\n",
            "Test Accuracy: 48.97119341563786%\n",
            "Training with learning rate: 0.001, batch size: 64, hidden size: 128, num layers: 2\n",
            "Epoch [1/10], Loss: 0.668433313186352\n",
            "Epoch [2/10], Loss: 0.6536988707689139\n",
            "Epoch [3/10], Loss: 0.6504546587283795\n",
            "Epoch [4/10], Loss: 0.6507724615243765\n",
            "Epoch [5/10], Loss: 0.6484438685270456\n",
            "Epoch [6/10], Loss: 0.6475452918272752\n",
            "Epoch [7/10], Loss: 0.6404337241099431\n",
            "Epoch [8/10], Loss: 0.6356814274421105\n",
            "Epoch [9/10], Loss: 0.6104410015619718\n",
            "Epoch [10/10], Loss: 0.6272622667826139\n",
            "Model saved to lstm_model_lr0.001_bs64_hs128_nl2.pth\n",
            "Test Accuracy: 48.559670781893004%\n",
            "Training with learning rate: 0.001, batch size: 64, hidden size: 256, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6620149612426758\n",
            "Epoch [2/10], Loss: 0.6531440844902625\n",
            "Epoch [3/10], Loss: 0.647589637682988\n",
            "Epoch [4/10], Loss: 0.6460073040081904\n",
            "Epoch [5/10], Loss: 0.6473632959219126\n",
            "Epoch [6/10], Loss: 0.6435961494079003\n",
            "Epoch [7/10], Loss: 0.670212814441094\n",
            "Epoch [8/10], Loss: 0.6467543152662424\n",
            "Epoch [9/10], Loss: 0.6458618549200205\n",
            "Epoch [10/10], Loss: 0.6448017083681546\n",
            "Model saved to lstm_model_lr0.001_bs64_hs256_nl1.pth\n",
            "Test Accuracy: 48.148148148148145%\n",
            "Training with learning rate: 0.001, batch size: 64, hidden size: 256, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6710864534744849\n",
            "Epoch [2/10], Loss: 0.6654971058552082\n",
            "Epoch [3/10], Loss: 0.654253519498385\n",
            "Epoch [4/10], Loss: 0.6499758958816528\n",
            "Epoch [5/10], Loss: 0.6459575983194205\n",
            "Epoch [6/10], Loss: 0.6416944632163415\n",
            "Epoch [7/10], Loss: 0.647895308641287\n",
            "Epoch [8/10], Loss: 0.6370492485853342\n",
            "Epoch [9/10], Loss: 0.6412392854690552\n",
            "Epoch [10/10], Loss: 0.6321953397530776\n",
            "Model saved to lstm_model_lr0.001_bs64_hs256_nl2.pth\n",
            "Test Accuracy: 49.794238683127574%\n",
            "Training with learning rate: 0.0001, batch size: 32, hidden size: 128, num layers: 1\n",
            "Epoch [1/10], Loss: 0.7025004739944751\n",
            "Epoch [2/10], Loss: 0.6899769283257998\n",
            "Epoch [3/10], Loss: 0.6775296788949233\n",
            "Epoch [4/10], Loss: 0.6629892060389886\n",
            "Epoch [5/10], Loss: 0.6518301413609431\n",
            "Epoch [6/10], Loss: 0.6512547249977405\n",
            "Epoch [7/10], Loss: 0.6495938071837792\n",
            "Epoch [8/10], Loss: 0.6482369693425986\n",
            "Epoch [9/10], Loss: 0.6476298570632935\n",
            "Epoch [10/10], Loss: 0.646329572567573\n",
            "Model saved to lstm_model_lr0.0001_bs32_hs128_nl1.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 32, hidden size: 128, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6978434255489936\n",
            "Epoch [2/10], Loss: 0.6785655090442071\n",
            "Epoch [3/10], Loss: 0.6582054931383866\n",
            "Epoch [4/10], Loss: 0.6524437757638785\n",
            "Epoch [5/10], Loss: 0.6517217250970694\n",
            "Epoch [6/10], Loss: 0.6521211541616\n",
            "Epoch [7/10], Loss: 0.6501390177469987\n",
            "Epoch [8/10], Loss: 0.6497422204567835\n",
            "Epoch [9/10], Loss: 0.6495234232682449\n",
            "Epoch [10/10], Loss: 0.6485153207412133\n",
            "Model saved to lstm_model_lr0.0001_bs32_hs128_nl2.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 32, hidden size: 256, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6951722433933845\n",
            "Epoch [2/10], Loss: 0.6750219716475561\n",
            "Epoch [3/10], Loss: 0.654958229798537\n",
            "Epoch [4/10], Loss: 0.6515679015563085\n",
            "Epoch [5/10], Loss: 0.6480488341588241\n",
            "Epoch [6/10], Loss: 0.6489361387032729\n",
            "Epoch [7/10], Loss: 0.6473535597324371\n",
            "Epoch [8/10], Loss: 0.6467800025756543\n",
            "Epoch [9/10], Loss: 0.644234680212461\n",
            "Epoch [10/10], Loss: 0.6452128497453836\n",
            "Model saved to lstm_model_lr0.0001_bs32_hs256_nl1.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 32, hidden size: 256, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6847004638268397\n",
            "Epoch [2/10], Loss: 0.653312277335387\n",
            "Epoch [3/10], Loss: 0.6528431796110593\n",
            "Epoch [4/10], Loss: 0.6530787692620204\n",
            "Epoch [5/10], Loss: 0.6510044657267057\n",
            "Epoch [6/10], Loss: 0.6496716187550471\n",
            "Epoch [7/10], Loss: 0.6500827761796805\n",
            "Epoch [8/10], Loss: 0.6476147793806516\n",
            "Epoch [9/10], Loss: 0.6455796200495499\n",
            "Epoch [10/10], Loss: 0.6427337343876178\n",
            "Model saved to lstm_model_lr0.0001_bs32_hs256_nl2.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 64, hidden size: 128, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6975841430517343\n",
            "Epoch [2/10], Loss: 0.6904022647784307\n",
            "Epoch [3/10], Loss: 0.6837043578808124\n",
            "Epoch [4/10], Loss: 0.6764198679190415\n",
            "Epoch [5/10], Loss: 0.6704304493390597\n",
            "Epoch [6/10], Loss: 0.6620957759710459\n",
            "Epoch [7/10], Loss: 0.6566755267289969\n",
            "Epoch [8/10], Loss: 0.6517707659648015\n",
            "Epoch [9/10], Loss: 0.6515067219734192\n",
            "Epoch [10/10], Loss: 0.6512425266779386\n",
            "Model saved to lstm_model_lr0.0001_bs64_hs128_nl1.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 64, hidden size: 128, num layers: 2\n",
            "Epoch [1/10], Loss: 0.7015585028208219\n",
            "Epoch [2/10], Loss: 0.6894062344844525\n",
            "Epoch [3/10], Loss: 0.6784810011203473\n",
            "Epoch [4/10], Loss: 0.6668525429872366\n",
            "Epoch [5/10], Loss: 0.657518730713771\n",
            "Epoch [6/10], Loss: 0.6515755745080801\n",
            "Epoch [7/10], Loss: 0.6516473247454717\n",
            "Epoch [8/10], Loss: 0.6515207703296955\n",
            "Epoch [9/10], Loss: 0.6507965968205378\n",
            "Epoch [10/10], Loss: 0.6514224089108981\n",
            "Model saved to lstm_model_lr0.0001_bs64_hs128_nl2.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 64, hidden size: 256, num layers: 1\n",
            "Epoch [1/10], Loss: 0.6781477974011347\n",
            "Epoch [2/10], Loss: 0.6712159606126639\n",
            "Epoch [3/10], Loss: 0.6652184495559106\n",
            "Epoch [4/10], Loss: 0.6590602076970614\n",
            "Epoch [5/10], Loss: 0.6529555045641385\n",
            "Epoch [6/10], Loss: 0.6523698430794936\n",
            "Epoch [7/10], Loss: 0.6511850998951838\n",
            "Epoch [8/10], Loss: 0.6501795053482056\n",
            "Epoch [9/10], Loss: 0.6501948099869949\n",
            "Epoch [10/10], Loss: 0.6493484194462116\n",
            "Model saved to lstm_model_lr0.0001_bs64_hs256_nl1.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Training with learning rate: 0.0001, batch size: 64, hidden size: 256, num layers: 2\n",
            "Epoch [1/10], Loss: 0.6901139204318707\n",
            "Epoch [2/10], Loss: 0.6739160922857431\n",
            "Epoch [3/10], Loss: 0.6572277729327862\n",
            "Epoch [4/10], Loss: 0.6526935192254874\n",
            "Epoch [5/10], Loss: 0.6514252194991479\n",
            "Epoch [6/10], Loss: 0.6509713347141559\n",
            "Epoch [7/10], Loss: 0.6499351446445172\n",
            "Epoch [8/10], Loss: 0.650972856925084\n",
            "Epoch [9/10], Loss: 0.6517011248148404\n",
            "Epoch [10/10], Loss: 0.650121441254249\n",
            "Model saved to lstm_model_lr0.0001_bs64_hs256_nl2.pth\n",
            "Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 128, Num Layers: 1, Test Accuracy: 48.97119341563786%\n",
            "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 128, Num Layers: 2, Test Accuracy: 51.851851851851855%\n",
            "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 256, Num Layers: 1, Test Accuracy: 46.91358024691358%\n",
            "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 256, Num Layers: 2, Test Accuracy: 44.8559670781893%\n",
            "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 128, Num Layers: 1, Test Accuracy: 48.97119341563786%\n",
            "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 128, Num Layers: 2, Test Accuracy: 48.559670781893004%\n",
            "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 256, Num Layers: 1, Test Accuracy: 48.148148148148145%\n",
            "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 256, Num Layers: 2, Test Accuracy: 49.794238683127574%\n",
            "Learning Rate: 0.0001, Batch Size: 32, Hidden Size: 128, Num Layers: 1, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 32, Hidden Size: 128, Num Layers: 2, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 32, Hidden Size: 256, Num Layers: 1, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 32, Hidden Size: 256, Num Layers: 2, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 64, Hidden Size: 128, Num Layers: 1, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 64, Hidden Size: 128, Num Layers: 2, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 64, Hidden Size: 256, Num Layers: 1, Test Accuracy: 47.73662551440329%\n",
            "Learning Rate: 0.0001, Batch Size: 64, Hidden Size: 256, Num Layers: 2, Test Accuracy: 47.73662551440329%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Experiment with different hyperparameters\n",
        "learning_rates = [0.001, 0.0001]\n",
        "batch_sizes = [32, 64]\n",
        "hidden_sizes = [128, 256]\n",
        "num_layers_list = [1, 2]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        for hs in hidden_sizes:\n",
        "            for nl in num_layers_list:\n",
        "                print(f'Training with learning rate: {lr}, batch size: {bs}, hidden size: {hs}, num layers: {nl}')\n",
        "\n",
        "                # Create DataLoader for training\n",
        "                train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True, collate_fn=collate_fn)\n",
        "                test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "                # Create and train the model\n",
        "                model = LSTMModel(vocab_size=len(train_dataset.word2idx), embedding_dim=100, hidden_size=hs, num_layers=nl, num_classes=len(set(train_dataset.labels)))\n",
        "                model = train_model(model, train_dataloader, lr)\n",
        "\n",
        "                # Save the model\n",
        "                save_model(model, lr, bs, hs, nl)\n",
        "\n",
        "                # Evaluate the model\n",
        "                accuracy = evaluate_model(model, test_dataloader)\n",
        "                results.append((lr, bs, hs, nl, accuracy))\n",
        "\n",
        "# Print results\n",
        "for lr, bs, hs, nl, acc in results:\n",
        "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Hidden Size: {hs}, Num Layers: {nl}, Test Accuracy: {acc}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjLrruTuhcs8"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tsv_file, tokenizer, max_length):\n",
        "        self.data = pd.read_csv(tsv_file, delimiter='\\t')\n",
        "        self.sentences = self.data['sentence'].values\n",
        "        self.labels = LabelEncoder().fit_transform(self.data['label'].values)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(dataset, batch_size):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "def train_model(model, train_dataloader, learning_rate, num_epochs=3):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_model(model, learning_rate, batch_size):\n",
        "    model_path = f'bert_model_lr{learning_rate}_bs{batch_size}.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'Model saved to {model_path}')\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy}%')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "max_length = 128\n",
        "train_dataset = TextDataset(train_tsv_file, tokenizer, max_length)\n",
        "test_dataset = TextDataset(test_tsv_file, tokenizer, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rates = [2e-5, 3e-5]\n",
        "batch_sizes = [16, 32]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        print(f'Training with learning rate: {lr}, batch size: {bs}')\n",
        "        \n",
        "        # Create DataLoader for training\n",
        "        train_dataloader = create_data_loader(train_dataset, bs)\n",
        "        test_dataloader = create_data_loader(test_dataset, bs)\n",
        "        \n",
        "        # Create and train the model\n",
        "        model = BERTModel(num_classes=len(set(train_dataset.labels)))\n",
        "        model = train_model(model, train_dataloader, lr)\n",
        "        \n",
        "        # Save the model\n",
        "        save_model(model, lr, bs)\n",
        "        \n",
        "        # Evaluate the model\n",
        "        accuracy = evaluate_model(model, test_dataloader)\n",
        "        results.append((lr, bs, accuracy))\n",
        "\n",
        "# Print results\n",
        "for lr, bs, acc in results:\n",
        "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Test Accuracy: {acc}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
